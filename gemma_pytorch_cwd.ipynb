{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the gemma model\n",
    "\n",
    "Exploratory analyses by Casey Dunn, [https://dunnlab.org]. I am running these analyses locally on a Windows 11 machine with CUDA 11.8.\n",
    "\n",
    "The goal is to take a look under the hood of the [Gemma](https://www.kaggle.com/models/google/gemma) model to better understand [transformers](https://arxiv.org/abs/1706.03762). In particular, I would like to have a practical understanding of the concepts explored in Grant Sanderson's series starting [here](https://youtu.be/wjZofJX0v4M?si=-Lsf1nH69cNyNVi3).\n",
    "\n",
    "## Importing model weights\n",
    "\n",
    "[Download](https://www.kaggle.com/models/google/gemma) the model weights for pytorch. This requires that you create a kaggle account, and you may need to request access to the model (approval can take a day or two). \n",
    "\n",
    "I used the  the 1.1-2b-it variation, and downloaded them as a as a `tar.gz` file. Decompress the file to get the `gemma-2b-it.ckpt` model weights file.\n",
    "\n",
    "## Configuring your environment\n",
    "\n",
    "Install the necessary packages\n",
    "\n",
    "```bash\n",
    "nvcc --version\n",
    "conda create --name torch\n",
    "conda activate torch\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 immutabledict sentencepiece matplotlib -c pytorch -c nvidia -c conda-forge\n",
    "```\n",
    "\n",
    "## Loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from gemma import config\n",
    "from gemma import model as gemma_model\n",
    "import random\n",
    "import numpy as np\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from https://ai.google.dev/gemma/docs/pytorch_gemma/ to implement the Gemma model in PyTorch. Also drawing from https://github.com/google/gemma_pytorch/blob/main/scripts/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '2b' # also works for '2b-it'\n",
    "MACHINE_TYPE = 'cuda'\n",
    "CKPT_PATH=\"C:\\\\Users\\\\casey\\\\Downloads\\\\gemma-2b-it.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config.get_model_config(VARIANT)\n",
    "model_config.dtype = \"float32\" if MACHINE_TYPE == \"cpu\" else \"float16\"\n",
    "model_config.quant = False\n",
    "\n",
    "# Seed random.\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "with set_default_tensor_type(model_config.get_dtype()):\n",
    "    model = gemma_model.GemmaForCausalLM(model_config)\n",
    "    model.load_weights(CKPT_PATH)\n",
    "    model = model.to(device).eval()\n",
    "print(\"Model loading done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the structure of the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are maxwell's equations?\"\n",
    "output_len = 200\n",
    "result = model.generate(prompt, device, output_len=output_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the tokenizer\n",
    "\n",
    "The tokenizer converts the input text into a sequence of integers, each representing a token. Sometimes a token is a whole word, sometimes it is a subword. A fun look at tokenizers can be found at [https://youtu.be/eLDYPvc7DNA?si=WPcVyjdCQlMe5cU0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the tokenizer from the model\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Input string\n",
    "input_string = \"This is an example sentence.\"\n",
    "\n",
    "# Tokenize the input string\n",
    "tokens = tokenizer.encode(input_string)\n",
    "\n",
    "# Print the resulting tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "  print(f\"\\t{token}\\t{tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"is\"))\n",
    "print(tokenizer.encode(\" is\"))\n",
    "print(tokenizer.encode(\"Is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_token_breaks(tokenizer, text):\n",
    "\ttokens = tokenizer.encode(text)\n",
    "\ttokens_text = list()\n",
    "\tfor token in tokens:\n",
    "\t\ttoken_text = tokenizer.decode(token)\n",
    "\t\ttokens_text.append(token_text)\n",
    "\treturn \"|\".join(tokens_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than look at lists of token integers and text, we can just mark the breaks between the tokens in the text with `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_token_breaks(tokenizer, \"This is an example sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_token_breaks(tokenizer, \"The gastrozooids of siphonophores are a type of polyp.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_token_breaks(tokenizer, \"let x_squared = x * x;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_token_breaks(tokenizer, \"y=mx+b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the embedder\n",
    "\n",
    "The embedder provides a vector embedding of each token. These vectors are the input to the transformer of the llm. \n",
    "\n",
    "First, let's copy the embeddings out of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embedder.weight.data\n",
    "\n",
    "# Move embeddings to the desired device (optional)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embeddings = embeddings.to(device)\n",
    "\n",
    "# Print the shape of the embeddings tensor\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are are 256000 unique tokens, each represented as a vector in $\\mathbb{R}^2048$ (2048 dimensional space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the euclidean lengths of the embeddings\n",
    "lengths = torch.linalg.norm(embeddings, ord=2, dim=1)\n",
    "lengths.shape\n",
    "print(lengths[0:20])\n",
    "\n",
    "# Plot a histogram of the lengths\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths.cpu().numpy(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a histogram of the lengths of all the embeddings. We can see that the embeddings are not all the same lengths, ie they do not fall on a sphere in $\\mathbb{R}^{2048}$. This means that embeddings differ by both the angle and the magnitude of the vector.\n",
    "\n",
    "This means that the cosine distance and the euclidean distance between embeddings carry different (though of course related) information. So we will examine both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_tokens_by_embedding(embeddings, embedding, n=5, cosine=True):\n",
    "\tif cosine:\n",
    "\t\t# Calculate the cosine similarities to all other embeddings\n",
    "\t\tsimilarities = torch.nn.functional.cosine_similarity(embeddings, embedding.unsqueeze(0), dim=1)\n",
    "\t\t# Get the indices of the n largest similarities\n",
    "\t\tclosest_tokens = torch.argsort(similarities, descending=True)[:n]\n",
    "\telse:\n",
    "\t\t# Calculate the euclidean distances to all other embeddings\n",
    "\t\tdistances = torch.linalg.norm(embeddings - embedding, ord=2, dim=1)\n",
    "\t\t# Get the indices of the n smallest distances\n",
    "\t\tclosest_tokens = torch.argsort(distances)[:n]\n",
    "\treturn closest_tokens\n",
    "\n",
    "\n",
    "# Function to find the n closest tokens to a given token based on euclidean distance in the embedding space\n",
    "def find_closest_tokens(embeddings, token, n=5, cosine=True):\n",
    "\t# Get the embedding for the token\n",
    "\tembedding = embeddings[token]\n",
    "\t\n",
    "\t# Calculate the euclidean distances to all other embeddings\n",
    "\tclosest_tokens = find_closest_tokens_by_embedding(embeddings, embedding, n, cosine)\n",
    "\treturn closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the embeddings in the neighborhood of the token for `chicken`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_token = tokenizer.encode(\"chicken\")[1]\n",
    "print(chicken_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, here are the closest tokens according to cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 closest tokens to the chicken token\n",
    "closest_tokens = find_closest_tokens(embeddings, chicken_token, n=20, cosine=True)\n",
    "print(closest_tokens)\n",
    "\n",
    "# Convert the tokens to text\n",
    "closest_tokens_text = [tokenizer.decode(int(token)) for token in closest_tokens]\n",
    "print(closest_tokens_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the closest tokens according to euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 closest tokens to the chicken token\n",
    "closest_tokens = find_closest_tokens(embeddings, chicken_token, n=20, cosine=False)\n",
    "print(closest_tokens)\n",
    "\n",
    "# Convert the tokens to text\n",
    "closest_tokens_text = [tokenizer.decode(int(token)) for token in closest_tokens]\n",
    "print(closest_tokens_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = tokenizer.encode(\"house\")[1]\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 closest tokens to the chicken token\n",
    "closest_tokens = find_closest_tokens(embeddings, token2, n=20, cosine=True)\n",
    "print(closest_tokens)\n",
    "\n",
    "# Convert the tokens to text\n",
    "closest_tokens_text = [tokenizer.decode(int(token)) for token in closest_tokens]\n",
    "print(closest_tokens_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 closest tokens to the chicken token\n",
    "closest_tokens = find_closest_tokens(embeddings, token2, n=20, cosine=False)\n",
    "print(closest_tokens)\n",
    "\n",
    "# Convert the tokens to text\n",
    "closest_tokens_text = [tokenizer.decode(int(token)) for token in closest_tokens]\n",
    "print(closest_tokens_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity might be a bit more interpretable than euclidean distance, and seems to be the norm in the field, so we will use that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy engine\n",
    "\n",
    "Each dimension in the embedding space can correspond to a different feature of the tokens. We can examine these relationships by subtracting the embeddings of two tokens, and applying that difference to another word. This is basically an analogy engine. For example, if we find the embedding of `king` - `man` + `woman`, we might get an embedding that is close to `queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_by_tokens(embeddings, token_a, token_b, token_c, n=5):\n",
    "\t# Get the embeddings for the tokens\n",
    "\tembedding_a = embeddings[token_a]\n",
    "\tembedding_b = embeddings[token_b]\n",
    "\tembedding_c = embeddings[token_c]\n",
    "\t\n",
    "\t# Calculate the analogy vector\n",
    "\tanalogy_vector = embedding_b - embedding_a + embedding_c\n",
    "\t\n",
    "\t# Calculate the euclidean distances to all other embeddings\n",
    "\tclosest_tokens = find_closest_tokens_by_embedding(embeddings, analogy_vector, n)\n",
    "\treturn closest_tokens\n",
    "\n",
    "def analogy(embeddings, tokenizer, text_a, text_b, text_c, n=5):\n",
    "\t# Encode the text to tokens\n",
    "\ttoken_a = tokenizer.encode(text_a)[1]\n",
    "\ttoken_b = tokenizer.encode(text_b)[1]\n",
    "\ttoken_c = tokenizer.encode(text_c)[1]\n",
    "\t\n",
    "\t# Calculate the analogy\n",
    "\tclosest_tokens = analogy_by_tokens(embeddings, token_a, token_b, token_c, n)\n",
    "\t\n",
    "\t# Convert the tokens to text\n",
    "\tclosest_tokens_text = [tokenizer.decode(int(token)) for token in closest_tokens]\n",
    "\n",
    "\treturn closest_tokens_text\n",
    "\n",
    "def token_distance(embeddings, token_a, token_b):\n",
    "\tembedding_a = embeddings[token_a]\n",
    "\tembedding_b = embeddings[token_b]\n",
    "\tdistance = torch.linalg.norm(embedding_a - embedding_b, ord=2)\n",
    "\treturn distance\n",
    "\n",
    "def text_distance(embeddings, tokenizer, text_a, text_b):\n",
    "\ttoken_a = tokenizer.encode(text_a)[1]\n",
    "\ttoken_b = tokenizer.encode(text_b)[1]\n",
    "\tdistance = token_distance(embeddings, token_a, token_b)\n",
    "\treturn distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(embeddings, tokenizer, \"man\", \"king\", \"woman\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(embeddings, tokenizer, \"man\", \"woman\", \"uncle\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(embeddings, tokenizer, \"finger\", \"hand\", \"petal\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(embeddings, tokenizer, \"fire\", \"burn\", \"flood\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(embeddings, tokenizer, \"apple\", \"apples\", \"banana\", 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_distance(embeddings, tokenizer, \"man\", \"woman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the whole, the analogy machine isn't great. It may be that without any sort of refinement from an attention mechanism, the raw embeddings are not so informative. King could mean a card, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local neighborhoods\n",
    "\n",
    "Above we just look at what tokens are nearby. Now let's look at the local neighborhood of a token in the embedding space. We will use PCA to reduce the dimensionality of the embeddings to 2D, and then plot the neighborhood of a token in this 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that given a token and embeddings, finds the n closest tokens based on cosine similarity.\n",
    "# It then returns a tuple with:\n",
    "# - A vector where the first element is the original token and the rest are the closest tokens.\n",
    "# - The subset of the embeddings corresponding to the tokens.\n",
    "def find_closest_tokens_by_cosine(embeddings, token, n=5):\n",
    "\t# Get the embedding for the token\n",
    "\tembedding = embeddings[token]\n",
    "\t\n",
    "\t# Calculate the cosine similarities to all other embeddings\n",
    "\tsimilarities = torch.nn.functional.cosine_similarity(embeddings, embedding.unsqueeze(0), dim=1)\n",
    "\t\n",
    "\t# Get the indices of the n largest similarities\n",
    "\tclosest_tokens = torch.argsort(similarities, descending=True)[:n]\n",
    "\t\n",
    "\t# Create a vector where the first element is the original token and the rest are the closest tokens\n",
    "\tclosest_tokens_vector = torch.cat([torch.tensor([token]), closest_tokens])\n",
    "\t\n",
    "\t# Get the subset of the embeddings corresponding to the tokens\n",
    "\tclosest_embeddings = embeddings[closest_tokens_vector]\n",
    "\treturn closest_tokens_vector, closest_embeddings\n",
    "\n",
    "# Function that given embeddings, runs a PCA on the embeddings and returns the first two principal components.\n",
    "def run_pca(embeddings):\n",
    "\t# Calculate the mean of the embeddings\n",
    "\tmean = torch.mean(embeddings, dim=0)\n",
    "\t\n",
    "\t# Subtract the mean from the embeddings\n",
    "\tcentered = embeddings - mean\n",
    "\t\n",
    "\t# Calculate the covariance matrix\n",
    "\tcov = torch.matmul(centered.T, centered)\n",
    "\t\n",
    "\t# Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "\teigenvalues, eigenvectors = torch.linalg.eig(cov)\n",
    "\t\n",
    "\t# Get the indices of the eigenvalues sorted in descending order\n",
    "\tindices = torch.argsort(eigenvalues[:, 0], descending=True)\n",
    "\t\n",
    "\t# Get the first two principal components\n",
    "\tpc1 = eigenvectors[:, indices[0]]\n",
    "\tpc2 = eigenvectors[:, indices[1]]\n",
    "\t\n",
    "\treturn pc1, pc2\n",
    "\n",
    "# Function that given a token and embeddings, runs find_closest_tokens_by_cosine() and run_pca() and plots the results.\n",
    "# tokens are converted to text using the tokenizer and added to the plot.\n",
    "def plot_closest_tokens_by_cosine(embeddings, tokenizer, token, n=5):\n",
    "\t# Find the closest tokens by cosine similarity\n",
    "\tclosest_tokens_vector, closest_embeddings = find_closest_tokens_by_cosine(embeddings, token, n)\n",
    "\t\n",
    "\t# Run PCA on the embeddings\n",
    "\tpc1, pc2 = run_pca(closest_embeddings)\n",
    "\t\n",
    "\t# Create a scatter plot of the embeddings\n",
    "\tplt.figure(figsize=(10, 10))\n",
    "\tplt.scatter(closest_embeddings[:, 0], closest_embeddings[:, 1])\n",
    "\t\n",
    "\t# Add the token text to the plot\n",
    "\tfor i, token in enumerate(closest_tokens_vector):\n",
    "\t\ttext = tokenizer.decode(int(token))\n",
    "\t\tplt.text(closest_embeddings[i, 0], closest_embeddings[i, 1], text)\n",
    "\t\n",
    "\t# Add the principal components to the plot\n",
    "\tplt.arrow(0, 0, pc1[0], pc1[1], color='r', width=0.01)\n",
    "\tplt.arrow(0, 0, pc2[0], pc2[1], color='g', width=0.01)\n",
    "\t\n",
    "\tplt.xlabel('Principal Component 1')\n",
    "\tplt.ylabel('Principal Component 2')\n",
    "\tplt.title('Closest Tokens by Cosine Similarity')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "A few incomeplete ideas.\n",
    "\n",
    "### infinite crafter\n",
    "\n",
    "Try to make something like https://neal.fun/infinite-craft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_combine_substances(substance_a, substance_b):\n",
    "\tprompt = f\"In one word describe the substance I get when I mix {substance_a} with {substance_b}.\"\n",
    "\toutput_len = 20\n",
    "\tresult = model.generate(prompt, device, output_len=output_len)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(craft_combine_substances(\"fire\", \"dirt\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
